{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievability Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrievability experiment and analysis of TREC678 corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps -\n",
    "* Generate query set Q containing unigram and bigram queries from the corpus\n",
    "* Perform retrieval for all queries q $\\in$ Q. If document d present within cutoff rank, then increament r(d) by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quert set Q divided into unigram queries and bigram queries\n",
    "# unigram queries are the corpus vocabulary terms which has tf>=5\n",
    "# vocab terms and their tf from lucene index\n",
    "# filter tf on the fly and add terms into a list\n",
    "\n",
    "# For bigram queries, use corpus doc generator class from doc2vec training code\n",
    "# From each doc, take bigrams and try adding them to a dictionary\n",
    "# if key found, add 1 to its value; if key not found, then add key into dict with value=1\n",
    "# this dict is bigram,tf pair\n",
    "# sort dict by tf in reverse order\n",
    "# take first 2 million bigram and make a list of these bigrams\n",
    "\n",
    "# write query set on disk for record\n",
    "\n",
    "# run loop over unigram and bigram queries\n",
    "# do BM25 retrieval of top c(=100) ranks for each query\n",
    "# iterate over top c docs and try adding them into r(d) dict\n",
    "# if key found, add 1 to its value; if key not found, then add key into dict with value=1\n",
    "# this dict is docid,occurenceCount\n",
    "# sort dict by value in ascending order\n",
    "\n",
    "# write r(d) dict on disk for record\n",
    "\n",
    "# plot Lorenz curve\n",
    "# compute Gini Coefficient G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.index import DirectoryReader\n",
    "from org.apache.lucene.index import MultiTerms\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.search.similarities import BM25Similarity\n",
    "from org.apache.lucene.search.similarities import LMJelinekMercerSimilarity\n",
    "from org.apache.lucene.search.similarities import LMDirichletSimilarity\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from java.io import File\n",
    "\n",
    "from org.apache.lucene.search import BooleanQuery\n",
    "from org.apache.lucene.search import BooleanClause\n",
    "from org.apache.lucene.search import TermQuery\n",
    "from org.apache.lucene.search import BoostQuery\n",
    "from org.apache.lucene.index import Term\n",
    "\n",
    "from org.apache.lucene.util import BytesRefIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f6f63b941f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this again if VM is not initialized already\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lucene index of the corpus\n",
    "index_path = '../index/'\n",
    "directory = FSDirectory.open(File(index_path).toPath())\n",
    "indexReader = DirectoryReader.open(directory)\n",
    "\n",
    "FIELDNAME = 'CONTENT'       # Lucene index field name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228501\n"
     ]
    }
   ],
   "source": [
    "unigram_queries = []\n",
    "\n",
    "terms = MultiTerms.getTerms(indexReader, FIELDNAME)\n",
    "iterator = terms.iterator()\n",
    "\n",
    "for term in BytesRefIterator.cast_(iterator):\n",
    "    term_str = term.utf8ToString()\n",
    "    t = Term(FIELDNAME, term_str)\n",
    "    tf = indexReader.totalTermFreq(t)\n",
    "    if tf >= 5:\n",
    "        unigram_queries.append(term_str)\n",
    "\n",
    "print(len(unigram_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write unigram_queries to disk\n",
    "with open('./unigram_queries.txt', 'w') as f:\n",
    "    f.write('\\n'.join(unigram_queries))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "class MyCorpus:\n",
    "    def __init__(self, corpusDirectory):\n",
    "        import os\n",
    "        # Corpus documents directory path\n",
    "        self.dirPath = corpusDirectory\n",
    "        self.fileNames = os.listdir(self.dirPath)\n",
    "        self.filePaths = [f'{self.dirPath}/{f}' for f in self.fileNames]\n",
    "        self.docCount = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        import re\n",
    "        tag_exp = re.compile('<.*?>')\n",
    "\n",
    "        def cleanTag(rawDoc):\n",
    "            cleanDoc = re.sub(tag_exp, '', rawDoc)\n",
    "            return cleanDoc\n",
    "\n",
    "        def process(oneDoc):\n",
    "            # global docCount\n",
    "            self.docCount += 1\n",
    "            # print(docCount, docid)   # weirdly, printing docid was contributing to RAM overflow\n",
    "            # print(docCount)\n",
    "            return cleanTag(oneDoc)\n",
    "        \n",
    "        # this function needs to be called for each of the files in the directory\n",
    "        def processFile(filePath):\n",
    "            with smart_open.open(filePath, 'r', encoding='ISO-8859-1') as f:\n",
    "                inDoc = False\n",
    "                docid,oneDoc = \"\",\"\"\n",
    "                docCounts,docids,contents = [],[],[]     # will store all the docs (docIDs, Contents) of a single file in a list\n",
    "                                            # with docid and contents in one-to-one list index-wise correspondence\n",
    "                                            # Why making lists? See the note in the next cell.\n",
    "                for line in f:\n",
    "                    if inDoc:\n",
    "                        if line.startswith(\"<DOCNO>\"):\n",
    "                            m = re.search('<DOCNO>(.+?)</DOCNO>', line)\n",
    "                            docid = m.group(1)\n",
    "                            continue\n",
    "                        elif line.strip() == \"</DOC>\":\n",
    "                            inDoc = False\n",
    "                            contents.append(process(oneDoc))\n",
    "                            docCounts.append(self.docCount)\n",
    "                            docids.append(docid.strip())\n",
    "                            oneDoc = \"\"\n",
    "                        else:\n",
    "                            oneDoc += line\n",
    "\n",
    "                    elif line.strip() == \"<DOC>\":\n",
    "                        inDoc = True\n",
    "                # return docids,contents\n",
    "                return docCounts,contents\n",
    "        \n",
    "        for filePath in self.filePaths:\n",
    "            integer_ids,contents = processFile(filePath)\n",
    "            for i in range(len(integer_ids)):\n",
    "                yield contents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upto #doc = 10000\n",
      "upto #doc = 20000\n",
      "upto #doc = 30000\n",
      "upto #doc = 40000\n",
      "upto #doc = 50000\n",
      "upto #doc = 60000\n",
      "upto #doc = 70000\n",
      "upto #doc = 80000\n",
      "upto #doc = 90000\n",
      "upto #doc = 100000\n",
      "upto #doc = 110000\n",
      "upto #doc = 120000\n",
      "upto #doc = 130000\n",
      "upto #doc = 140000\n",
      "upto #doc = 150000\n",
      "upto #doc = 160000\n",
      "upto #doc = 170000\n",
      "upto #doc = 180000\n",
      "upto #doc = 190000\n",
      "upto #doc = 200000\n",
      "upto #doc = 210000\n",
      "upto #doc = 220000\n",
      "upto #doc = 230000\n",
      "upto #doc = 240000\n",
      "upto #doc = 250000\n",
      "upto #doc = 260000\n",
      "upto #doc = 270000\n",
      "upto #doc = 280000\n",
      "upto #doc = 290000\n",
      "upto #doc = 300000\n",
      "upto #doc = 310000\n",
      "upto #doc = 320000\n",
      "upto #doc = 330000\n",
      "upto #doc = 340000\n",
      "upto #doc = 350000\n",
      "upto #doc = 360000\n",
      "upto #doc = 370000\n",
      "upto #doc = 380000\n",
      "upto #doc = 390000\n",
      "upto #doc = 400000\n",
      "upto #doc = 410000\n",
      "upto #doc = 420000\n",
      "upto #doc = 430000\n",
      "upto #doc = 440000\n",
      "upto #doc = 450000\n",
      "upto #doc = 460000\n",
      "upto #doc = 470000\n",
      "upto #doc = 480000\n",
      "upto #doc = 490000\n",
      "upto #doc = 500000\n",
      "upto #doc = 510000\n",
      "upto #doc = 520000\n",
      "\n",
      "Number of docs covered during bigram sampling = 528155\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.regexp import blankline_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Corpus documents directory path\n",
    "dirPath = '../../TREC_67/documents_robust04/'\n",
    "trec_corpus = MyCorpus(corpusDirectory=dirPath)\n",
    "\n",
    "bigram_counter = Counter()      # to store bigrams with their frequencies\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"'s\"])\n",
    "stopwords_set = set(stop_words)\n",
    "\n",
    "count_doc = 0\n",
    "for doc in trec_corpus:\n",
    "    count_doc += 1\n",
    "    # text pre-processing:\n",
    "    # blankline tokenization, then sentence tokenization, then word tokenization\n",
    "    sents_nested = [sent_tokenize(ss) for ss in blankline_tokenize(doc)]\n",
    "    sents = [sent for sublist in sents_nested for sent in sublist]\n",
    "    tokens_nested = [word_tokenize(s) for s in sents]\n",
    "    # stopword removal and all char non-alphanumeric token removal\n",
    "    tokens_nested = [[token for token in tokens if any(char.isalnum() for char in token) and token.lower() not in stopwords_set] for tokens in tokens_nested]\n",
    "    # bigram sampling one sentence at a time\n",
    "    for tokens in tokens_nested:\n",
    "        bi_grams = ngrams(tokens, 2)\n",
    "        bigram_counter.update(bi_grams)\n",
    "    # stdout count_doc logging\n",
    "    if count_doc%10000==0:\n",
    "        print(f'upto #doc = {count_doc}')\n",
    "\n",
    "print(f'\\nNumber of docs covered during bigram sampling = {count_doc}')\n",
    "\n",
    "limit = 2000000     # 2 million -> #bigram limit\n",
    "bigram_list = sorted([x for x in bigram_counter.items() if x[1]>=20], reverse=True, key=lambda lst: lst[1])[:limit]\n",
    "bigram_queries = [' '.join(x[0]) for x in bigram_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write bigram_queries to disk\n",
    "with open('./bigram_queries.txt', 'w') as f:\n",
    "    f.write('\\n'.join(bigram_queries))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797452"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievals and estimating Document Retrievability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram queries run starts...\n",
      "Unigram queries progress...  4.38%\n",
      "Unigram queries progress...  8.75%\n",
      "Unigram queries progress...  13.13%\n",
      "Unigram queries progress...  17.51%\n",
      "Unigram queries progress...  21.88%\n",
      "Unigram queries progress...  26.26%\n",
      "Unigram queries progress...  30.63%\n",
      "Unigram queries progress...  35.01%\n",
      "Unigram queries progress...  39.39%\n",
      "Unigram queries progress...  43.76%\n",
      "Unigram queries progress...  48.14%\n",
      "Unigram queries progress...  52.52%\n",
      "Unigram queries progress...  56.89%\n",
      "Unigram queries progress...  61.27%\n",
      "Unigram queries progress...  65.65%\n",
      "Unigram queries progress...  70.02%\n",
      "Unigram queries progress...  74.40%\n",
      "Unigram queries progress...  78.77%\n",
      "Unigram queries progress...  83.15%\n",
      "Unigram queries progress...  87.53%\n",
      "Unigram queries progress...  91.90%\n",
      "Unigram queries progress...  96.28%\n",
      "Run completed with unigram queries progress...  100.00%\n",
      "\n",
      "Bigram queries run starts...\n",
      "Bigram queries progress...  1.25%\n",
      "Bigram queries progress...  2.51%\n",
      "Bigram queries progress...  3.76%\n",
      "Bigram queries progress...  5.02%\n",
      "Bigram queries progress...  6.27%\n",
      "Bigram queries progress...  7.52%\n",
      "Bigram queries progress...  8.78%\n",
      "Bigram queries progress...  10.03%\n",
      "Bigram queries progress...  11.29%\n",
      "Bigram queries progress...  12.54%\n",
      "Bigram queries progress...  13.79%\n",
      "Bigram queries progress...  15.05%\n",
      "Bigram queries progress...  16.30%\n",
      "Bigram queries progress...  17.56%\n",
      "Bigram queries progress...  18.81%\n",
      "Bigram queries progress...  20.06%\n",
      "Bigram queries progress...  21.32%\n",
      "Bigram queries progress...  22.57%\n",
      "Bigram queries progress...  23.83%\n",
      "Bigram queries progress...  25.08%\n",
      "Bigram queries progress...  26.33%\n",
      "Bigram queries progress...  27.59%\n",
      "Bigram queries progress...  28.84%\n",
      "Bigram queries progress...  30.10%\n",
      "Bigram queries progress...  31.35%\n",
      "Bigram queries progress...  32.60%\n",
      "Bigram queries progress...  33.86%\n",
      "Bigram queries progress...  35.11%\n",
      "Bigram queries progress...  36.37%\n",
      "Bigram queries progress...  37.62%\n",
      "Bigram queries progress...  38.87%\n",
      "Bigram queries progress...  40.13%\n",
      "Bigram queries progress...  41.38%\n",
      "Bigram queries progress...  42.64%\n",
      "Bigram queries progress...  43.89%\n",
      "Bigram queries progress...  45.14%\n",
      "Bigram queries progress...  46.40%\n",
      "Bigram queries progress...  47.65%\n",
      "Bigram queries progress...  48.91%\n",
      "Bigram queries progress...  50.16%\n",
      "Bigram queries progress...  51.41%\n",
      "Bigram queries progress...  52.67%\n",
      "Bigram queries progress...  53.92%\n",
      "Bigram queries progress...  55.18%\n",
      "Bigram queries progress...  56.43%\n",
      "Bigram queries progress...  57.68%\n",
      "Bigram queries progress...  58.94%\n",
      "Bigram queries progress...  60.19%\n",
      "Bigram queries progress...  61.45%\n",
      "Bigram queries progress...  62.70%\n",
      "Bigram queries progress...  63.95%\n",
      "Bigram queries progress...  65.21%\n",
      "Bigram queries progress...  66.46%\n",
      "Bigram queries progress...  67.72%\n",
      "Bigram queries progress...  68.97%\n",
      "Bigram queries progress...  70.22%\n",
      "Bigram queries progress...  71.48%\n",
      "Bigram queries progress...  72.73%\n",
      "Bigram queries progress...  73.99%\n",
      "Bigram queries progress...  75.24%\n",
      "Bigram queries progress...  76.49%\n",
      "Bigram queries progress...  77.75%\n",
      "Bigram queries progress...  79.00%\n",
      "Bigram queries progress...  80.26%\n",
      "Bigram queries progress...  81.51%\n",
      "Bigram queries progress...  82.76%\n",
      "Bigram queries progress...  84.02%\n",
      "Bigram queries progress...  85.27%\n",
      "Bigram queries progress...  86.53%\n",
      "Bigram queries progress...  87.78%\n",
      "Bigram queries progress...  89.03%\n",
      "Bigram queries progress...  90.29%\n",
      "Bigram queries progress...  91.54%\n",
      "Bigram queries progress...  92.80%\n",
      "Bigram queries progress...  94.05%\n",
      "Bigram queries progress...  95.30%\n",
      "Bigram queries progress...  96.56%\n",
      "Bigram queries progress...  97.81%\n",
      "Bigram queries progress...  99.07%\n",
      "Run completed with bigram queries progress...  348.99%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# setting up the searcher\n",
    "analyzer = EnglishAnalyzer()    # used same analyzer as indexer\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "\n",
    "model = 'bm25'\n",
    "k1 = 0.8; b = 0.4\n",
    "similarityModel = BM25Similarity(k1,b)\n",
    "# setting the similarity model\n",
    "searcher.setSimilarity(similarityModel)\n",
    "\n",
    "r_d = Counter()     # to store lucene_docids and their cumulative counts\n",
    "\n",
    "\n",
    "def retrieve(query):\n",
    "    escaped_q = QueryParser('CONTENT', analyzer).escape(query)\n",
    "    q = QueryParser('CONTENT', analyzer).parse(escaped_q)\n",
    "    \n",
    "    # getting the top c search results using the searcher\n",
    "    c = 100\n",
    "    scoreDocs = searcher.search(q, c).scoreDocs\n",
    "        \n",
    "    lucene_docids = []\n",
    "    for scoreDoc in scoreDocs:\n",
    "        lucene_docids.append(scoreDoc.doc)\n",
    "            \n",
    "    r_d.update(lucene_docids)\n",
    "\n",
    "print('Unigram queries run starts...')\n",
    "with open('./unigram_queries.txt') as f:\n",
    "    i = 0\n",
    "    len_unigram_queries = 228501\n",
    "    for line in f:\n",
    "        query = line[:-1]\n",
    "        retrieve(query)\n",
    "        \n",
    "        i += 1\n",
    "        if i%10000==0:\n",
    "            print(f'Unigram queries progress... {i*100/len_unigram_queries: .2f}%')\n",
    "        \n",
    "    print(f'Run completed with unigram queries progress... {i*100/len_unigram_queries: .2f}%\\n')\n",
    "\n",
    "print('Bigram queries run starts...')\n",
    "with open('./bigram_queries.txt') as f:\n",
    "    i = 0\n",
    "    len_bigram_queries = 797452\n",
    "    for line in f:\n",
    "        query = line[:-1]\n",
    "        retrieve(query)\n",
    "        \n",
    "        i += 1\n",
    "        if i%10000==0:\n",
    "            print(f'Bigram queries progress... {i*100/len_bigram_queries: .2f}%')\n",
    "    \n",
    "    print(f'Run completed with bigram queries progress... {i*100/len_bigram_queries: .2f}%')\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./rd.pickle', 'wb') as f:\n",
    "    pickle.dump(r_d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed with bigram queries progress...  100.00%\n"
     ]
    }
   ],
   "source": [
    "print(f'Run completed with bigram queries progress... {i*100/len_bigram_queries: .2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./rd.pickle', 'rb') as f:\n",
    "    r_d_load = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r_d_load.most_common(10)\n",
    "type(r_d_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(250940, 12719),\n",
       " (290694, 7146),\n",
       " (290697, 4076),\n",
       " (290693, 3562),\n",
       " (262627, 3441),\n",
       " (70208, 3112),\n",
       " (398846, 3017),\n",
       " (266912, 2930),\n",
       " (150269, 2820),\n",
       " (87389, 2765)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_d.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc=210200 score=1.9868847 shardIndex=0\n",
      "doc=19831 score=1.9738262 shardIndex=0\n",
      "doc=515136 score=1.9668773 shardIndex=0\n",
      "doc=130628 score=1.961197 shardIndex=0\n",
      "doc=286465 score=1.9611716 shardIndex=0\n",
      "doc=516459 score=1.9607632 shardIndex=0\n",
      "doc=216507 score=1.9568355 shardIndex=0\n",
      "doc=216494 score=1.955946 shardIndex=0\n",
      "doc=12444 score=1.9508271 shardIndex=0\n",
      "doc=286457 score=1.950824 shardIndex=0\n"
     ]
    }
   ],
   "source": [
    "# setting up the searcher\n",
    "analyzer = EnglishAnalyzer()    # used same analyzer as indexer\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "\n",
    "def ret(query):\n",
    "    q = QueryParser('CONTENT', analyzer).parse(query)\n",
    "    \n",
    "    # getting the top c search results using the searcher\n",
    "    c = 10\n",
    "    scoreDocs = searcher.search(q, c).scoreDocs\n",
    "        \n",
    "    docids = []\n",
    "    for scoreDoc in scoreDocs:\n",
    "        print(scoreDoc)\n",
    "        doc = searcher.doc(scoreDoc.doc)\n",
    "        docids.append(doc.get('DOCID'))\n",
    "\n",
    "ret('United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FBIS3-38143'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.doc(0).get('DOCID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
