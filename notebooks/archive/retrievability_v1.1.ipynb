{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievability Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrievability experiment and analysis of TREC678 corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps -\n",
    "* Generate query set Q containing unigram and bigram queries from the corpus\n",
    "* Perform retrieval for all queries q $\\in$ Q. If document d present within cutoff rank, then increament r(d) by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quert set Q divided into unigram queries and bigram queries\n",
    "# unigram queries are the corpus vocabulary terms which has tf>=5\n",
    "# vocab terms and their tf from lucene index\n",
    "# filter tf on the fly and add terms into a list\n",
    "\n",
    "# For bigram queries, use corpus doc generator class from doc2vec training code\n",
    "# From each doc, take bigrams and try adding them to a dictionary\n",
    "# if key found, add 1 to its value; if key not found, then add key into dict with value=1\n",
    "# this dict is bigram,tf pair\n",
    "# sort dict by tf in reverse order\n",
    "# take first 2 million bigram and make a list of these bigrams\n",
    "\n",
    "# write query set on disk for record\n",
    "\n",
    "# run loop over unigram and bigram queries\n",
    "# do BM25 retrieval of top c(=100) ranks for each query\n",
    "# iterate over top c docs and try adding them into r(d) dict\n",
    "# if key found, add 1 to its value; if key not found, then add key into dict with value=1\n",
    "# this dict is docid,occurenceCount\n",
    "# sort dict by value in ascending order\n",
    "\n",
    "# write r(d) dict on disk for record\n",
    "\n",
    "# plot Lorenz curve\n",
    "# compute Gini Coefficient G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucene\n",
    "from org.apache.lucene.search import IndexSearcher\n",
    "from org.apache.lucene.index import DirectoryReader\n",
    "from org.apache.lucene.index import MultiTerms\n",
    "from org.apache.lucene.store import FSDirectory\n",
    "from org.apache.lucene.queryparser.classic import QueryParser\n",
    "from org.apache.lucene.search.similarities import BM25Similarity\n",
    "from org.apache.lucene.search.similarities import LMJelinekMercerSimilarity\n",
    "from org.apache.lucene.search.similarities import LMDirichletSimilarity\n",
    "from org.apache.lucene.analysis.en import EnglishAnalyzer\n",
    "from java.io import File\n",
    "\n",
    "from org.apache.lucene.search import BooleanQuery\n",
    "from org.apache.lucene.search import BooleanClause\n",
    "from org.apache.lucene.search import TermQuery\n",
    "from org.apache.lucene.search import BoostQuery\n",
    "from org.apache.lucene.index import Term\n",
    "\n",
    "from org.apache.lucene.util import BytesRefIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jcc.JCCEnv at 0x7f2067b1d350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this again if VM is not initialized already\n",
    "lucene.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lucene index of the corpus\n",
    "index_path = '../../Dwaipayan sir/3. Rocchio & RM3/index/'\n",
    "directory = FSDirectory.open(File(index_path).toPath())\n",
    "indexReader = DirectoryReader.open(directory)\n",
    "\n",
    "FIELDNAME = 'CONTENT'       # Lucene index field name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228501\n"
     ]
    }
   ],
   "source": [
    "unigram_queries = []\n",
    "\n",
    "terms = MultiTerms.getTerms(indexReader, FIELDNAME)\n",
    "iterator = terms.iterator()\n",
    "\n",
    "for term in BytesRefIterator.cast_(iterator):\n",
    "    term_str = term.utf8ToString()\n",
    "    t = Term(FIELDNAME, term_str)\n",
    "    tf = indexReader.totalTermFreq(t)\n",
    "    if tf >= 5:\n",
    "        unigram_queries.append(term_str)\n",
    "\n",
    "print(len(unigram_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write unigram_queries to disk\n",
    "with open('./unigram_queries.txt', 'w') as f:\n",
    "    f.write('\\n'.join(unigram_queries))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "class MyCorpus:\n",
    "    def __init__(self, corpusDirectory):\n",
    "        import os\n",
    "        # Corpus documents directory path\n",
    "        self.dirPath = corpusDirectory\n",
    "        self.fileNames = os.listdir(self.dirPath)\n",
    "        self.filePaths = [f'{self.dirPath}/{f}' for f in self.fileNames]\n",
    "        self.docCount = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        import re\n",
    "        tag_exp = re.compile('<.*?>')\n",
    "\n",
    "        def cleanTag(rawDoc):\n",
    "            cleanDoc = re.sub(tag_exp, '', rawDoc)\n",
    "            return cleanDoc\n",
    "\n",
    "        def process(oneDoc):\n",
    "            # global docCount\n",
    "            self.docCount += 1\n",
    "            # print(docCount, docid)   # weirdly, printing docid was contributing to RAM overflow\n",
    "            # print(docCount)\n",
    "            return cleanTag(oneDoc)\n",
    "        \n",
    "        # this function needs to be called for each of the files in the directory\n",
    "        def processFile(filePath):\n",
    "            with smart_open.open(filePath, 'r', encoding='ISO-8859-1') as f:\n",
    "                inDoc = False\n",
    "                docid,oneDoc = \"\",\"\"\n",
    "                docCounts,docids,contents = [],[],[]     # will store all the docs (docIDs, Contents) of a single file in a list\n",
    "                                            # with docid and contents in one-to-one list index-wise correspondence\n",
    "                                            # Why making lists? See the note in the next cell.\n",
    "                for line in f:\n",
    "                    if inDoc:\n",
    "                        if line.startswith(\"<DOCNO>\"):\n",
    "                            m = re.search('<DOCNO>(.+?)</DOCNO>', line)\n",
    "                            docid = m.group(1)\n",
    "                            continue\n",
    "                        elif line.strip() == \"</DOC>\":\n",
    "                            inDoc = False\n",
    "                            contents.append(process(oneDoc))\n",
    "                            docCounts.append(self.docCount)\n",
    "                            docids.append(docid.strip())\n",
    "                            oneDoc = \"\"\n",
    "                        else:\n",
    "                            oneDoc += line\n",
    "\n",
    "                    elif line.strip() == \"<DOC>\":\n",
    "                        inDoc = True\n",
    "                # return docids,contents\n",
    "                return docCounts,contents\n",
    "        \n",
    "        for filePath in self.filePaths:\n",
    "            integer_ids,contents = processFile(filePath)\n",
    "            for i in range(len(integer_ids)):\n",
    "                yield contents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.regexp import blankline_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Corpus documents directory path\n",
    "dirPath = '../../Dwaipayan sir/3. Rocchio & RM3/trec678_corpus/documents'\n",
    "trec_corpus = MyCorpus(corpusDirectory=dirPath)\n",
    "\n",
    "bigram_counter = Counter()      # to store bigrams with their frequencies\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"'s\"])\n",
    "stopwords_set = set(stop_words)\n",
    "\n",
    "count_doc = 0\n",
    "for doc in trec_corpus:\n",
    "    count_doc += 1\n",
    "    # text pre-processing:\n",
    "    # blankline tokenization, then sentence tokenization, then word tokenization\n",
    "    sents_nested = [sent_tokenize(ss) for ss in blankline_tokenize(doc)]\n",
    "    sents = [sent for sublist in sents_nested for sent in sublist]\n",
    "    tokens_nested = [word_tokenize(s) for s in sents]\n",
    "    # stopword removal and all char non-alphanumeric token removal\n",
    "    tokens_nested = [[token for token in tokens if any(char.isalnum() for char in token) and token.lower() not in stopwords_set] for tokens in tokens_nested]\n",
    "    # bigram sampling one sentence at a time\n",
    "    for tokens in tokens_nested:\n",
    "        bi_grams = ngrams(tokens, 2)\n",
    "        bigram_counter.update(bi_grams)\n",
    "    # stdout count_doc logging\n",
    "    if count_doc%10000==0:\n",
    "        print(f'upto #doc = {count_doc}')\n",
    "\n",
    "print(f'\\nNumber of docs covered during bigram sampling = {count_doc}')\n",
    "\n",
    "limit = 2000000     # 2 million -> #bigram limit\n",
    "bigram_list = sorted([x for x in bigram_counter.items() if x[1]>=20], reverse=True, key=lambda lst: lst[1])[:limit]\n",
    "bigram_queries = [' '.join(x[0]) for x in bigram_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write bigram_queries to disk\n",
    "with open('./bigram_queries.txt', 'w') as f:\n",
    "    f.write('\\n'.join(bigram_queries))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievals and estimating Document Retrievability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# setting up the searcher\n",
    "analyzer = EnglishAnalyzer()    # used same analyzer as indexer\n",
    "searcher = IndexSearcher(DirectoryReader.open(directory))\n",
    "\n",
    "model = 'bm25'\n",
    "k1 = 0.8; b = 0.4\n",
    "similarityModel = BM25Similarity(k1,b)\n",
    "# setting the similarity model\n",
    "searcher.setSimilarity(similarityModel)\n",
    "\n",
    "r_d = Counter()     # to store DOCIDs and their cumulative counts\n",
    "\n",
    "\n",
    "def retrieve(query):\n",
    "    q = QueryParser('CONTENT', analyzer).parse(query)\n",
    "    \n",
    "    # getting the top c search results using the searcher\n",
    "    c = 100\n",
    "    scoreDocs = searcher.search(q, c).scoreDocs\n",
    "        \n",
    "    docids = []\n",
    "    for scoreDoc in scoreDocs:\n",
    "        doc = searcher.doc(scoreDoc.doc)\n",
    "        docids.append(doc.get('DOCID'))\n",
    "            \n",
    "    r_d.update(docids)\n",
    "\n",
    "\n",
    "with open('./unigram_queries.txt') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        query = line[:-1]\n",
    "        retrieve(query)\n",
    "        \n",
    "        i += 1\n",
    "        if i >= 1000:\n",
    "            break\n",
    "\n",
    "with open('./bigram_queries.txt') as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        query = line[:-1]\n",
    "        retrieve(query)\n",
    "        \n",
    "        i += 1\n",
    "        if i >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FR940527-1-00176', 228),\n",
       " ('FR940505-1-00477', 110),\n",
       " ('FR940505-1-00478', 103),\n",
       " ('FR941116-0-00016', 94),\n",
       " ('FR940803-1-00006', 94),\n",
       " ('FR940527-1-00157', 80),\n",
       " ('FR940919-0-00127', 62),\n",
       " ('FR940919-0-00064', 60),\n",
       " ('FBIS3-42440', 57),\n",
       " ('FR940527-1-00159', 57)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_d.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc=229601 score=2.0238037 shardIndex=0\n",
      "doc=19831 score=2.0236964 shardIndex=0\n",
      "doc=210200 score=2.013215 shardIndex=0\n",
      "doc=286457 score=2.010394 shardIndex=0\n",
      "doc=286465 score=2.0103028 shardIndex=0\n",
      "doc=12444 score=2.0081882 shardIndex=0\n",
      "doc=130628 score=2.00703 shardIndex=0\n",
      "doc=68729 score=2.006886 shardIndex=0\n",
      "doc=6789 score=2.003496 shardIndex=0\n",
      "doc=43671 score=2.0031536 shardIndex=0\n"
     ]
    }
   ],
   "source": [
    "def ret(query):\n",
    "    q = QueryParser('CONTENT', analyzer).parse(query)\n",
    "    \n",
    "    # getting the top c search results using the searcher\n",
    "    c = 10\n",
    "    scoreDocs = searcher.search(q, c).scoreDocs\n",
    "        \n",
    "    docids = []\n",
    "    for scoreDoc in scoreDocs:\n",
    "        print(scoreDoc)\n",
    "        doc = searcher.doc(scoreDoc.doc)\n",
    "        docids.append(doc.get('DOCID'))\n",
    "\n",
    "ret('United States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FBIS3-1'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher.doc(7368).get('DOCID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus2:\n",
    "    def __init__(self, corpusDirectory):\n",
    "        import os\n",
    "        # Corpus documents directory path\n",
    "        self.dirPath = corpusDirectory\n",
    "        self.fileNames = os.listdir(self.dirPath)\n",
    "        self.filePaths = [f'{self.dirPath}/{f}' for f in self.fileNames]\n",
    "        self.docCount = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        import re\n",
    "        tag_exp = re.compile('<.*?>')\n",
    "\n",
    "        def cleanTag(rawDoc):\n",
    "            cleanDoc = re.sub(tag_exp, '', rawDoc)\n",
    "            return cleanDoc\n",
    "\n",
    "        def process(oneDoc):\n",
    "            # global docCount\n",
    "            self.docCount += 1\n",
    "            # print(docCount, docid)   # weirdly, printing docid was contributing to RAM overflow\n",
    "            # print(docCount)\n",
    "            return cleanTag(oneDoc)\n",
    "        \n",
    "        # this function needs to be called for each of the files in the directory\n",
    "        def processFile(filePath):\n",
    "            with smart_open.open(filePath, 'r', encoding='ISO-8859-1') as f:\n",
    "                inDoc = False\n",
    "                docid,oneDoc = \"\",\"\"\n",
    "                docCounts,docids,contents = [],[],[]     # will store all the docs (docIDs, Contents) of a single file in a list\n",
    "                                            # with docid and contents in one-to-one list index-wise correspondence\n",
    "                                            # Why making lists? See the note in the next cell.\n",
    "                for line in f:\n",
    "                    if inDoc:\n",
    "                        if line.startswith(\"<DOCNO>\"):\n",
    "                            m = re.search('<DOCNO>(.+?)</DOCNO>', line)\n",
    "                            docid = m.group(1)\n",
    "                            continue\n",
    "                        elif line.strip() == \"</DOC>\":\n",
    "                            inDoc = False\n",
    "                            contents.append(process(oneDoc))\n",
    "                            docCounts.append(self.docCount)\n",
    "                            docids.append(docid.strip())\n",
    "                            oneDoc = \"\"\n",
    "                        else:\n",
    "                            oneDoc += line\n",
    "\n",
    "                    elif line.strip() == \"<DOC>\":\n",
    "                        inDoc = True\n",
    "                # return docids,contents\n",
    "                return docCounts,contents,docids\n",
    "        \n",
    "        for filePath in self.filePaths:\n",
    "            integer_ids,contents,docIDs = processFile(filePath)\n",
    "            for i in range(len(integer_ids)):\n",
    "                yield integer_ids[i],docIDs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docCount\tdocid\tdocid_lucene\n",
      "7368\tFBIS3-7689\tFBIS3-1\n"
     ]
    }
   ],
   "source": [
    "dirPath = '../../Dwaipayan sir/3. Rocchio & RM3/trec678_corpus/documents'\n",
    "corpus = MyCorpus2(corpusDirectory=dirPath)\n",
    "\n",
    "i = 0\n",
    "print('docCount\\tdocid\\tdocid_lucene')\n",
    "for x in corpus:\n",
    "    docCount,docid = x\n",
    "    # print(f\"{docCount}\\t{docid}\\t{searcher.doc(docCount).get('DOCID')}\")\n",
    "    i += 1\n",
    "    if i%10000==0:\n",
    "        print(i)\n",
    "    if searcher.doc(docCount).get('DOCID') == 'FBIS3-1':\n",
    "        print(f\"{docCount}\\t{docid}\\t{searcher.doc(docCount).get('DOCID')}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
